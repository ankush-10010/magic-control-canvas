# -*- coding: utf-8 -*-
"""working_stage1_lora_integrator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d0P2D6uuJ2H1r3lbBbcKejX9qeF88A3I

Features till now are basic ,
  lora integration is possible ,
  their sensitivities can be changed ,
  manual seed is set so to compare between the generated images
  as everytime with the same prompt we will get the same image
"""

!pip install fastapi uvicorn diffusers transformers peft accelerate torch safetensors

!pip install pyngrok

from huggingface_hub import notebook_login
notebook_login()

import torch
import gc
gc.collect()
torch.cuda.empty_cache()

from fastapi import FastAPI, HTTPException, UploadFile, Form
from fastapi.responses import FileResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, DDIMScheduler
from transformers import CLIPTextModel, CLIPTokenizer
import torch
import os
# from peft import PeftMode
app = FastAPI()

# Allow cross-origin requests for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For dev; restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load base model components
# base_path = "/content/stable-diffusion-v1-5"
# base_path ="stabilityai/stable-diffusion-xl-base-1.0"
base_path="runwayml/stable-diffusion-v1-5"
# base_path="xyn-ai/anything-v4.0"
vae = AutoencoderKL.from_pretrained(base_path, subfolder="vae", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained(base_path, subfolder="text_encoder", torch_dtype=torch.float16)
tokenizer = CLIPTokenizer.from_pretrained(base_path, subfolder="tokenizer")
unet = UNet2DConditionModel.from_pretrained(base_path, subfolder="unet", torch_dtype=torch.float16)
scheduler = DDIMScheduler.from_pretrained(base_path, subfolder="scheduler")

pipe = StableDiffusionPipeline(
    vae=vae,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    unet=unet,
    scheduler=scheduler,
    safety_checker=None,
    feature_extractor=None,
    requires_safety_checker=False
).to("cuda")  # Ensure CUDA is available, else use "cpu"
# pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")
# Optional compilation
# pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")

import os
import re
from fastapi import Form, HTTPException
from huggingface_hub import snapshot_download

@app.post("/load-lora/")
async def load_lora(lora_path: str = Form(...)):
    try:
        # Check if it's a Hugging Face repo path (contains '/')
        if '/' in lora_path:
            # Download/cache the repository locally
            local_path = snapshot_download(repo_id=lora_path, repo_type="model")
        else:
            # It's already a local path
            local_path = lora_path

        # Find all safetensors files in the directory
        safetensors_files = [f for f in os.listdir(local_path) if f.endswith('.safetensors')]

        if not safetensors_files:
            raise HTTPException(status_code=404, detail="No safetensors files found in the specified path")

        # If only one file, use it directly
        if len(safetensors_files) == 1:
            best_file = safetensors_files[0]
        else:
            # Function to extract step number from filename
            def extract_step_number(fname):
                # Look for patterns like: -000032.safetensors, -1500.safetensors, etc.
                match = re.search(r"-(\d+)\.safetensors$", fname)
                return int(match.group(1)) if match else -1

            # Select the file with the highest step number
            best_file = max(safetensors_files, key=extract_step_number)

            # Delete all other safetensors files, keep only the best one
            for file in safetensors_files:
                if file != best_file:
                    file_path = os.path.join(local_path, file)
                    try:
                        os.remove(file_path)
                        print(f"üóëÔ∏è Deleted: {file}")
                    except Exception as delete_error:
                        print(f"‚ö†Ô∏è Warning: Could not delete {file}: {delete_error}")

        # Construct full path to the best file
        best_file_path = os.path.join(local_path, best_file)

        print(f"üöÄ Loading LoRA: {best_file_path}")

        # Load the LoRA weights
        pipe.load_lora_weights(local_path)

        return {
            "message": f"Successfully loaded {best_file} from {lora_path}",
            "loaded_file": best_file,
            "local_path": local_path
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load LoRA: {str(e)}")

# Prompt input model
class PromptRequest(BaseModel):
    prompt: str
    height: int = 512
    width: int = 512
    num_inference_steps: int = 20
    guidance_scale: float = 7.5

import traceback
import asyncio
from concurrent.futures import ThreadPoolExecutor
from fastapi.responses import FileResponse
from io import BytesIO
from fastapi.responses import StreamingResponse
executor = ThreadPoolExecutor()

def generate_image_sync(prompt, height, width, steps, scale):
    generator = torch.manual_seed(42)
    return pipe(
        prompt=prompt,
        height=height,
        width=width,
        num_inference_steps=steps,
        guidance_scale=scale,
        generator=generator
    ).images[0]


@app.post("/generate-image/")
async def generate(request: PromptRequest):
  try:
      print("‚öôÔ∏è Generating with prompt:", request.prompt)
      print("Prompt:", request.prompt)
      print("Width:", request.width, "Height:", request.height)
      print("Steps:", request.num_inference_steps, "Scale:", request.guidance_scale)
      print("Checkpoint 0")
      image = await asyncio.get_event_loop().run_in_executor(
          executor,
          generate_image_sync,
          request.prompt,
          request.height,
          request.width,
          request.num_inference_steps,
          request.guidance_scale
      )
      print("Checkpoint 1")
      # image_path = "/content/output_lora.png"
      # image.save(image_path)
      # return FileResponse(image_path, media_type="image/png", filename="output_lora.png")
      img_bytes = BytesIO()
      image.save(img_bytes, format="PNG")
      img_bytes.seek(0)
      print("Bytes length:", len(img_bytes.getvalue()))
      return StreamingResponse(
          img_bytes,
          media_type="image/png",
          headers={
              "Content-Disposition": 'inline; filename="output.png"',
              "Content-Type": "image/png"
          }
      )
  except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
import psutil

def get_cpu_ram_stats():
    ram = psutil.virtual_memory()
    cpu = psutil.cpu_percent(interval=1)
    return {
        "cpu_percent": cpu,
        "ram_total": ram.total,
        "ram_used": ram.used,
        "ram_percent": ram.percent
    }
import subprocess
import re

def get_gpu_stats():
    try:
        result = subprocess.check_output(
            ['nvidia-smi', '--query-gpu=memory.total,memory.used,utilization.gpu', '--format=csv,nounits,noheader']
        )
        total, used, utilization = map(int, result.decode().strip().split(', '))
        return {
            "gpu_memory_total": total,
            "gpu_memory_used": used,
            "gpu_utilization": utilization
        }
    except Exception as e:
        return {"error": str(e)}
@app.get("/stats")
def get_stats():
    return {
        "cpu_ram": get_cpu_ram_stats(),
        "gpu": get_gpu_stats()
    }
gc.collect()
torch.cuda.empty_cache()

import uvicorn
from pyngrok import ngrok
import nest_asyncio
!ngrok authtoken 2xNBjsJZTEMSyxxxxxxxauqUwXAxK_3YEAxPqFauzLBSHA8EJvn
# Allow async in Colab
nest_asyncio.apply()

# Start ngrok tunnel
public_url = ngrok.connect(8000)
print("Public URL:", public_url)

# Run FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)

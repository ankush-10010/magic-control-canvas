# -*- coding: utf-8 -*-
"""LoRAs_integration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X6pkxVmtsasWaxX1UdcGrwThJjGHekk3
"""
"""
This one is just for adding LoRAs for SD1.5 which have only one safetensor file
Purpose of adding this file is to have a file which takes the image from the backend and 
send it to frontend page , had a lots of debugging to achieve this thing so i uploaded this here
"""
!pip install fastapi uvicorn diffusers transformers peft accelerate torch safetensors

from diffusers import StableDiffusionPipeline

# This will automatically download all required components into the cache
pipeline = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0")

# Save it locally for custom loading
pipeline.save_pretrained("/content/stable-diffusion-xl-base-1.0")

from fastapi import FastAPI, HTTPException, UploadFile, Form
from fastapi.responses import FileResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, DDIMScheduler
from transformers import CLIPTextModel, CLIPTokenizer
import torch
import os
# from peft import PeftMode
app = FastAPI()

# Allow cross-origin requests for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For dev; restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load base model components
# base_path = "/content/stable-diffusion-v1-5"
base_path ="runwayml/stable-diffusion-v1-5"
vae = AutoencoderKL.from_pretrained(base_path, subfolder="vae", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained(base_path, subfolder="text_encoder", torch_dtype=torch.float16)
tokenizer = CLIPTokenizer.from_pretrained(base_path, subfolder="tokenizer")
unet = UNet2DConditionModel.from_pretrained(base_path, subfolder="unet", torch_dtype=torch.float16)
scheduler = DDIMScheduler.from_pretrained(base_path, subfolder="scheduler")

pipe = StableDiffusionPipeline(
    vae=vae,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    unet=unet,
    scheduler=scheduler,
    safety_checker=None,
    feature_extractor=None,
    requires_safety_checker=False
).to("cuda")  # Ensure CUDA is available, else use "cpu"
# pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")
# Optional compilation
# pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")

from diffusers import StableDiffusionPipeline
import torch

prompt = "Extreme close-up of a single tiger eye, direct frontal view. Detailed iris and pupil. Sharp focus on eye texture and color. Natural lighting to capture authentic eye shine and depth. The word 'FLUX' is painted over it in big, white brush strokes with visible texture."

print("Checkpoint 0")

try:
    result = pipe(
        prompt=prompt,
        height=512,
        width=512,
        num_inference_steps=20,
        guidance_scale=7.5
    )
    print("Checkpoint1")
    image = result.images[0]
    image.save("output_lora.png")
    print("✅ Saved as output_lora.png")
except Exception as e:
    print("❌ Failed to generate image:", str(e))

# LoRA loading endpoint  ("this is for basic LoRA loading, not for LoRA with multiple safetensors")
# This endpoint allows you to load a LoRA model from a specified path.
# @app.post("/load-lora/")
# async def load_lora(
#     lora_path: str = Form(...)
# ):
#     try:
#         pipe.load_lora_weights(lora_path)
#         return {"message": f"LoRA loaded from {lora_path}"}
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"Failed to load LoRA: {str(e)}")
    
from fastapi import Form
from fastapi import HTTPException
import os
import re
"""this is for LoRA with multiple safetensors files ,
 which are usually the case in hugging face lora models
"""

@app.post("/load-lora/")
async def load_lora(lora_dir: str = Form(...)):
    try:
        if not os.path.exists(lora_dir):
            raise ValueError(f"Directory {lora_dir} does not exist")

        safetensors_files = [f for f in os.listdir(lora_dir) if f.endswith(".safetensors")]
        if not safetensors_files:
            raise ValueError("No .safetensors files found in the provided directory.")

        # Pick the one with highest step number
        def extract_step(filename):
            match = re.search(r"-(\d+)\.safetensors$", filename)
            return int(match.group(1)) if match else -1

        best_file = max(safetensors_files, key=extract_step)
        best_file_path = os.path.join(lora_dir, best_file)

        pipe.load_lora_weights(best_file_path)
        return {"message": f"Loaded best LoRA: {best_file}"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load LoRA: {str(e)}")

# Prompt input model
class PromptRequest(BaseModel):
    prompt: str
    height: int = 512
    width: int = 512
    num_inference_steps: int = 20
    guidance_scale: float = 7.5

# Image generation endpoint
# @app.post("/generate-image/")
# async def generate_image(request: PromptRequest):
#     image_path = "output_lora.png"
#     try:
#         image = pipe(
#             prompt=request.prompt,
#             height=request.height,
#             width=request.width,
#             num_inference_steps=request.num_inference_steps,
#             guidance_scale=request.guidance_scale
#         ).images[0]

#         image.save(image_path)
#         return FileResponse(image_path, media_type="image/png")
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))
#     finally:
#         if os.path.exists(image_path):
#             os.remove(image_path)
import traceback

@app.post("/generate-image/")
async def generate_image(request: PromptRequest):
    image_path = "/content/output_lora.png"
    try:
        print("⚙️ Generating with prompt:", request.prompt)
        print("Prompt:", request.prompt)
        print("Width:", request.width, "Height:", request.height)
        print("Steps:", request.num_inference_steps, "Scale:", request.guidance_scale)
        print("Checkpoint 0")
        image = pipe(
            prompt=request.prompt,
            height=request.height,
            width=request.width,
            num_inference_steps=request.num_inference_steps,
            guidance_scale=request.guidance_scale,
        ).images[0]
        print("⏳ Image generation complete.")
        print("Image list:", image.images)
        print("Checkpoint1")
        image.show()  # Display in notebook
        image.save(image_path)
        return FileResponse(image_path, media_type="image/png")
    except Exception as e:
        print("❌ Exception occurred:")
        traceback.print_exc()  # ← VERY IMPORTANT
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if os.path.exists(image_path):
            os.remove(image_path)

# LoRA loading endpoint
@app.post("/load-lora/")
async def load_lora(
    lora_path: str = Form(...)
):
    try:
        pipe.load_lora_weights(lora_path)
        return {"message": f"LoRA loaded from {lora_path}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load LoRA: {str(e)}")


# Prompt input model
class PromptRequest(BaseModel):
    prompt: str
    height: int = 512
    width: int = 512
    num_inference_steps: int = 20
    guidance_scale: float = 7.5

import traceback
import asyncio
from concurrent.futures import ThreadPoolExecutor
from fastapi.responses import FileResponse
from io import BytesIO
from fastapi.responses import StreamingResponse
executor = ThreadPoolExecutor()

def generate_image_sync(prompt, height, width, steps, scale):
    return pipe(
        prompt=prompt,
        height=height,
        width=width,
        num_inference_steps=steps,
        guidance_scale=scale
    ).images[0]


@app.post("/generate-image/")
async def generate(request: PromptRequest):
  try:
      print("⚙️ Generating with prompt:", request.prompt)
      print("Prompt:", request.prompt)
      print("Width:", request.width, "Height:", request.height)
      print("Steps:", request.num_inference_steps, "Scale:", request.guidance_scale)
      print("Checkpoint 0")
      image = await asyncio.get_event_loop().run_in_executor(
          executor,
          generate_image_sync,
          request.prompt,
          request.height,
          request.width,
          request.num_inference_steps,
          request.guidance_scale
      )
      print("Checkpoint 1")
      # image_path = "/content/output_lora.png"
      # image.save(image_path)
      # return FileResponse(image_path, media_type="image/png", filename="output_lora.png")
      img_bytes = BytesIO()
      image.save(img_bytes, format="PNG")
      img_bytes.seek(0)
      print("Bytes length:", len(img_bytes.getvalue()))
      return StreamingResponse(
          img_bytes,
          media_type="image/png",
          headers={
              "Content-Disposition": 'inline; filename="output.png"',
              "Content-Type": "image/png"
          }
      )
  except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# from fastapi.responses import FileResponse
# import uuid
# import os

# def generate_image_sync(prompt, height, width, steps, scale):
#     image = pipe(
#         prompt=prompt,
#         height=height,
#         width=width,
#         num_inference_steps=steps,
#         guidance_scale=scale
#     ).images[0]
#     return image

# @app.post("/generate-image/")
# async def generate(request: PromptRequest):
#     print("⚙️ Generating with prompt:", request.prompt)
#     print("Prompt:", request.prompt)
#     print("Width:", request.width, "Height:", request.height)
#     print("Steps:", request.num_inference_steps, "Scale:", request.guidance_scale)
#     print("Checkpoint 0")

#     image = await asyncio.get_event_loop().run_in_executor(
#         executor,
#         generate_image_sync,
#         request.prompt,
#         request.height,
#         request.width,
#         request.num_inference_steps,
#         request.guidance_scale
#     )

#     print("Checkpoint 1")

#     # ✅ Use a unique filename to avoid browser caching issues
#     filename = f"output_{uuid.uuid4().hex}.png"
#     image_path = f"/content/{filename}"
#     image.save(image_path, format="PNG")

#     # ✅ Confirm the file is saved and readable
#     if not os.path.exists(image_path):
#         raise RuntimeError(f"Image not saved at {image_path}")

#     print(f"✅ Saved image at: {image_path}")
#     return FileResponse(path=image_path, media_type="image/png", filename=filename)

!pip install pyngrok

from huggingface_hub import notebook_login
notebook_login()

import torch
import gc
gc.collect()
torch.cuda.empty_cache()

import uvicorn
from pyngrok import ngrok
import nest_asyncio
!ngrok authtoken <"ngrok-api-key">
# !ngrok authtoken 2xNBjsJZTEMSyZIjuMauqUwXAxK_3YPAxPqFauzLSSHA8EJvn  (something like this)
# Allow async in Colab
nest_asyncio.apply()

# Start ngrok tunnel
public_url = ngrok.c3onnect(8000)
print("Public URL:", public_url)

# Run FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)

# -*- coding: utf-8 -*-
"""backend_controlnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sFpVHqb4kBydgtyrtGZ20v_QJeM-uWi7
"""

# !pip install -q diffusers==0.14.0 transformers xformers git+https://github.com/huggingface/accelerate.git
# !pip install -q controlnet_aux
# !pip install -q opencv-contrib-python
# !pip install fastapi uvicorn

# !pip install diffusers

import torch
import gc
gc.collect()
torch.cuda.empty_cache()

from huggingface_hub import notebook_login
notebook_login()

!pip install fastapi uvicorn diffusers transformers peft accelerate torch safetensors

# !pip install huggingface_hub==0.16.4

# !pip install --upgrade huggingface_hub
# !pip install --upgrade diffusers
# !pip install --upgrade transformers

from fastapi import FastAPI, HTTPException, UploadFile, Form ,status , File
from fastapi.responses import FileResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from diffusers import StableDiffusionControlNetPipeline, UNet2DConditionModel, AutoencoderKL, DDIMScheduler , ControlNetModel
from transformers import CLIPTextModel, CLIPTokenizer
import torch
import os

# from peft import PeftMode
app = FastAPI()

# Allow cross-origin requests for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For dev; restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

from typing import Optional, Dict

active_controlnets: Dict[str, ControlNetModel] = {}  # Tracks loaded ControlNets

# Global store for frontend inputs
stored_inputs: Dict[str, Dict[str, Optional[str]]] = {}

@app.post("/load-controlnet/")
async def load_controlnet(
    controlnet_path: str = Form(..., description="Path to ControlNet/ControlLoRA model"),
    adapter_name: Optional[str] = Form(None, description="Unique name for model management"),
    lora_weights_path: Optional[str] = Form(None, description="Required for ControlLoRA"),
    torch_dtype: Optional[str] = Form("float16", description="float16 or float32")
):
    # Generate a unique key to identify this input
    model_key = "controlnet"

    # Store input values in global variable
    stored_inputs[model_key] = {
        "controlnet_path": controlnet_path,
        "adapter_name": adapter_name,
        "lora_weights_path": lora_weights_path,
        "torch_dtype": torch_dtype
    }

    return {"status": "stored", "adapter_name": model_key}

import traceback
import asyncio
from concurrent.futures import ThreadPoolExecutor
from fastapi.responses import FileResponse
from io import BytesIO
from fastapi.responses import StreamingResponse
executor = ThreadPoolExecutor()



from fastapi import HTTPException, UploadFile, File
from typing import Dict, Optional
import asyncio
from io import BytesIO
from PIL import Image
from functools import partial
import torch
from torchvision.transforms import ToTensor

from diffusers import (
    StableDiffusionPipeline,
    ControlNetModel, StableDiffusionControlNetPipeline,
    StableDiffusionInpaintPipeline
)

@app.post("/generate-image/")
async def generate( prompt: str = Form(...),
    height: int = Form(...),
    width: int = Form(...),
    num_inference_steps: int = Form(...),
    guidance_scale: float = Form(...),
    control_image: Optional[UploadFile] = File(None)):
    print("Prompt:", prompt)
    print("Height:", height)
    print("Width:", width)
    print("Num Inference Steps:", num_inference_steps)
    print("Guidance Scale:", guidance_scale)
    print("Control Image:", control_image.filename if control_image else None)
    print("checkpoint 0")
    try:
        print("checkpoint 1")
        # print(f"‚öôÔ∏è Generating with ControlNet: {pipe.controlnet is not None}")
        # Suppose you want to retrieve a specific model's inputs:
        model_key = "controlnet"  # Use the key you stored it with
        if model_key in stored_inputs:
            controlnet_path = stored_inputs[model_key]["controlnet_path"]
            adapter_name = stored_inputs[model_key]["adapter_name"]
            torch_dtype="float16"
            print(f"ControlNet path: {controlnet_path}")
            print(f"Adapter name: {adapter_name}")
            adapter_name = f"controlnet_{adapter_name or os.path.basename(controlnet_path)}"
            # Validate inputs
            dtype = torch.float16 if torch_dtype == "float16" else torch.float32
            # Load ControlNet/ControlLoRA
            controlnet = ControlNetModel.from_pretrained(
                controlnet_path,
                torch_dtype=dtype
            ).to("cuda")
            model_key = adapter_name or controlnet_path
            active_controlnets[model_key] = controlnet
        else:
            controlnet=None
            dtype=None
            print(f"No stored input found for key: {model_key}")
        base_model_path="runwayml/stable-diffusion-v1-5"
        if controlnet is not None and control_image is not None:
                print("Entered block with CN and CI")
                pipe = StableDiffusionControlNetPipeline.from_pretrained(
                base_model_path,
                controlnet=controlnet,
                torch_dtype=torch_dtype,
                safety_checker=None,
                feature_extractor=None
            ).to("cuda")

        # Case 2: No ControlNet, just base model
        elif controlnet is None:
            print("Entered block without CN")
            pipe = StableDiffusionPipeline.from_pretrained(
                base_model_path,
                torch_dtype="float16",
                safety_checker=None,
                feature_extractor=None
            ).to("cuda")

        # Case 3: ControlNet but no Control Image ‚Äì invalid
        elif controlnet is not None and control_image is None:
            raise ValueError("Control image must be provided when using ControlNet.")
        def generate_image_sync(prompt, height, width, num_inference_steps, guidance_scale,control_image_pil=None):
            try:
              generator = torch.manual_seed(42)
              return pipe(
                  prompt=prompt,
                  height=height,
                  width=width,
                  num_inference_steps=num_inference_steps,
                  guidance_scale=guidance_scale,
                  image=control_image_pil,
                  generator=generator,
              ).images[0]
            except Exception as e:
                  print("‚ùå Exception inside generate_image_sync:", e)
                  import traceback
                  traceback.print_exc()
                  raise
        # Process control image if provided
        print("Checkpoint fnxn pass")
        control_image_pil = None
        if control_image:
            image_bytes = await control_image.read()
            control_image_pil = Image.open(BytesIO(image_bytes)).convert("RGB")
            # control_image_pil = ToTensor()(control_image_pil).unsqueeze(0)  # Add batch dimension
        #     print("chckpnt23")
        #     print("Control image tensor device: %s", control_image_pil.device)
            # control_image_pil = control_image_pil.to("cuda")  # Move to GPU
        #     # control_image_pil = Image.open(BytesIO(await control_image.read()))
        print("checkpoint 2")
        print("üß™ Type of control_image_pil:", type(control_image_pil))
        # print("üßæ Is controlnet loaded?", pipe.controlnet is not None)
        print("üñºÔ∏è Final control image type:", type(control_image_pil))
        # print("Control image tensor device: %s", control_image_pil.device)
        # # Run in executor
        fn = partial(
                      generate_image_sync,
                      prompt=prompt,
                      height=height,
                      width=width,
                      num_inference_steps=num_inference_steps,
                      guidance_scale=guidance_scale,
                      control_image_pil=control_image_pil  # Explicitly name the parameter
                  )
        image = await asyncio.get_event_loop().run_in_executor(executor, fn)
        print("checkpoint 3")
        # Stream response
        img_bytes = BytesIO()
        image.save(img_bytes, format="PNG")
        img_bytes.seek(0)
        return StreamingResponse(
            img_bytes,
            media_type="image/png",
            headers={"Content-Disposition": 'inline; filename="output.png"'}
        )
    except Exception as e:
        print(f"‚ùå Generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/active-controlnets/")
async def list_controlnets():
    return {"active_controlnets": list(active_controlnets.keys())}

@app.delete("/unload-controlnet/")
async def unload_controlnet(
    model_key: str = Form(..., description="Name or path of model to unload")
):
    try:
        if model_key in active_controlnets:
            del active_controlnets[model_key]
            return {"message": f"Unloaded {model_key}"}
        return {"message": "Model not found"}, 404
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/pipeline-state/")
async def get_state():
    return {
        "pipeline_type": pipe.__class__.__name__,
        "active_controlnet": pipe.controlnet is not None,
        "active_adapters": list(pipe.get_active_adapters()),
    }

import os
import re
from fastapi import Form, HTTPException
from huggingface_hub import snapshot_download

@app.post("/load-lora/")
async def load_lora(
    lora_path: str = Form(...),
    adapter_name: str = Form("default"),
    lora_scale: float = Form(1.0)
    ):
    try:
        adapter_name=f"lora_{adapter_name}"
        # Check if it's a Hugging Face repo path (contains '/')
        if '/' in lora_path:
            # Download/cache the repository locally
            local_path = snapshot_download(repo_id=lora_path, repo_type="model")
        else:
            # It's already a local path
            local_path = lora_path

        # Find all safetensors files in the directory
        safetensors_files = [f for f in os.listdir(local_path) if f.endswith('.safetensors')]

        if not safetensors_files:
            raise HTTPException(status_code=404, detail="No safetensors files found in the specified path")

        # If only one file, use it directly
        if len(safetensors_files) == 1:
            best_file = safetensors_files[0]
        else:
            # Function to extract step number from filename
            def extract_step_number(fname):
                # Look for patterns like: -000032.safetensors, -1500.safetensors, etc.
                match = re.search(r"-(\d+)\.safetensors$", fname)
                return int(match.group(1)) if match else -1

            # Select the file with the highest step number
            best_file = max(safetensors_files, key=extract_step_number)

            # Delete all other safetensors files, keep only the best one
            for file in safetensors_files:
                if file != best_file:
                    file_path = os.path.join(local_path, file)
                    try:
                        os.remove(file_path)
                        print(f"üóëÔ∏è Deleted: {file}")
                    except Exception as delete_error:
                        print(f"‚ö†Ô∏è Warning: Could not delete {file}: {delete_error}")

        # Construct full path to the best file
        best_file_path = os.path.join(local_path, best_file)

        print(f"üöÄ Loading LoRA: {best_file_path}")
        if not hasattr(pipe, "load_lora_weights"):
            raise HTTPException(status_code=400, detail="Pipeline doesn't support LoRA")
        # Load the LoRA weights
        pipe.load_lora_weights(local_path,adapter_name=adapter_name)

        return {
            "message": f"Successfully loaded {best_file} from {lora_path}",
            "loaded_file": best_file,
            "local_path": local_path
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load LoRA: {str(e)}")

import psutil

def get_cpu_ram_stats():
    ram = psutil.virtual_memory()
    cpu = psutil.cpu_percent(interval=1)
    return {
        "cpu_percent": cpu,
        "ram_total": ram.total,
        "ram_used": ram.used,
        "ram_percent": ram.percent
    }

import subprocess
import re

def get_gpu_stats():
    try:
        result = subprocess.check_output(
            ['nvidia-smi', '--query-gpu=memory.total,memory.used,utilization.gpu', '--format=csv,nounits,noheader']
        )
        total, used, utilization = map(int, result.decode().strip().split(', '))
        return {
            "gpu_memory_total": total,
            "gpu_memory_used": used,
            "gpu_utilization": utilization
        }
    except Exception as e:
        return {"error": str(e)}

@app.get("/stats")
def get_stats():
    return {
        "cpu_ram": get_cpu_ram_stats(),
        "gpu": get_gpu_stats()
    }

!pip install pyngrok

import torch
import gc

gc.collect()
torch.cuda.empty_cache()

import uvicorn
from pyngrok import ngrok
import nest_asyncio
!ngrok authtoken 2xNBjsJZTEMSyZIjuNauqUwXAxK_3YEAxPqFauzLBSHA8EJvn
# Allow async in Colab
nest_asyncio.apply()

# Start ngrok tunnel
public_url = ngrok.connect(8000)
print("Public URL:", public_url)

# Run FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)


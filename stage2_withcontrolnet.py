# -*- coding: utf-8 -*-
"""Stage2_withControlNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JyA_QgN4X29NlU_OcRMEjev52e2ueVlP
"""
"""
The generate image endpoint is not working , its not able to request data from the backend probably , so have to fix it first , apart from that everything works
"""
!pip install fastapi uvicorn diffusers transformers peft accelerate torch safetensors

import torch
import gc
gc.collect()
torch.cuda.empty_cache()

from huggingface_hub import notebook_login
notebook_login()

from fastapi import FastAPI, HTTPException, UploadFile, Form ,status , File
from fastapi.responses import FileResponse
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from diffusers import StableDiffusionControlNetPipeline, UNet2DConditionModel, AutoencoderKL, DDIMScheduler , ControlNetModel
from transformers import CLIPTextModel, CLIPTokenizer
import torch
import os
# from peft import PeftMode
app = FastAPI()

# Allow cross-origin requests for frontend communication
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For dev; restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load base model components
# base_path = "/content/stable-diffusion-v1-5"
# base_path ="stabilityai/stable-diffusion-xl-base-1.0"
base_path="runwayml/stable-diffusion-v1-5"
# base_path="xyn-ai/anything-v4.0"
vae = AutoencoderKL.from_pretrained(base_path, subfolder="vae", torch_dtype=torch.float16)
text_encoder = CLIPTextModel.from_pretrained(base_path, subfolder="text_encoder", torch_dtype=torch.float16)
tokenizer = CLIPTokenizer.from_pretrained(base_path, subfolder="tokenizer")
unet = UNet2DConditionModel.from_pretrained(base_path, subfolder="unet", torch_dtype=torch.float16)
scheduler = DDIMScheduler.from_pretrained(base_path, subfolder="scheduler")

pipe = StableDiffusionControlNetPipeline(
    vae=vae,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    unet=unet,
    scheduler=scheduler,
    safety_checker=None,
    feature_extractor=None,
    requires_safety_checker=False,
    controlnet=None,
).to("cuda")  # Ensure CUDA is available, else use "cpu"
# pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")
# Optional compilation
# pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")

from typing import Optional, Dict

active_controlnets: Dict[str, ControlNetModel] = {}  # Tracks loaded ControlNets

@app.post("/load-controlnet/")
async def load_controlnet(
    controlnet_path: str = Form(..., description="Path to ControlNet/ControlLoRA model"),
    adapter_name: Optional[str] = Form(None, description="Unique name for model management"),
    lora_weights_path: Optional[str] = Form(None, description="Required for ControlLoRA"),
    torch_dtype: Optional[str] = Form("float16", description="float16 or float32")
):

    try:
        adapter_name = f"controlnet_{adapter_name or os.path.basename(controlnet_path)}"
        # Validate inputs
        dtype = torch.float16 if torch_dtype == "float16" else torch.float32

        # Load ControlNet/ControlLoRA
        controlnet = ControlNetModel.from_pretrained(
            controlnet_path,
            torch_dtype=dtype
        )
        model_key = adapter_name or controlnet_path
        active_controlnets[model_key] = controlnet

        pipe.controlnet = controlnet
        # For ControlLoRA: Inject LoRA weights
        if lora_weights_path:
            if not hasattr(pipe, "load_lora_weights"):
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Pipeline doesn't support LoRA injection"
                )
            pipe.load_lora_weights(
                lora_weights_path,
                adapter_name=model_key
            )
        return {
            "message": f"ControlNet loaded as '{adapter_name}'",
            "active_controlnet": pipe.controlnet is not None,
            "active_adapters": list(pipe.get_active_adapters())
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ControlNet loading failed: {str(e)}"
        )

@app.get("/active-controlnets/")
async def list_controlnets():
    return {"active_controlnets": list(active_controlnets.keys())}

@app.delete("/unload-controlnet/")
async def unload_controlnet(
    model_key: str = Form(..., description="Name or path of model to unload")
):
    try:
        if model_key in active_controlnets:
            del active_controlnets[model_key]
            # If this was the active ControlNet, reset pipeline
            if pipe.controlnet == active_controlnets.get(model_key):
                pipe.controlnet = None
            return {"message": f"Unloaded {model_key}"}
        return {"message": "Model not found"}, 404
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/pipeline-state/")
async def get_state():
    return {
        "pipeline_type": pipe.__class__.__name__,
        "active_controlnet": pipe.controlnet is not None,
        "active_adapters": list(pipe.get_active_adapters()),
    }

import os
import re
from fastapi import Form, HTTPException
from huggingface_hub import snapshot_download

@app.post("/load-lora/")
async def load_lora(
    lora_path: str = Form(...),
    adapter_name: str = Form("default"),
    lora_scale: float = Form(1.0)
    ):
    try:
        adapter_name=f"lora_{adapter_name}"
        # Check if it's a Hugging Face repo path (contains '/')
        if '/' in lora_path:
            # Download/cache the repository locally
            local_path = snapshot_download(repo_id=lora_path, repo_type="model")
        else:
            # It's already a local path
            local_path = lora_path

        # Find all safetensors files in the directory
        safetensors_files = [f for f in os.listdir(local_path) if f.endswith('.safetensors')]

        if not safetensors_files:
            raise HTTPException(status_code=404, detail="No safetensors files found in the specified path")

        # If only one file, use it directly
        if len(safetensors_files) == 1:
            best_file = safetensors_files[0]
        else:
            # Function to extract step number from filename
            def extract_step_number(fname):
                # Look for patterns like: -000032.safetensors, -1500.safetensors, etc.
                match = re.search(r"-(\d+)\.safetensors$", fname)
                return int(match.group(1)) if match else -1

            # Select the file with the highest step number
            best_file = max(safetensors_files, key=extract_step_number)

            # Delete all other safetensors files, keep only the best one
            for file in safetensors_files:
                if file != best_file:
                    file_path = os.path.join(local_path, file)
                    try:
                        os.remove(file_path)
                        print(f"üóëÔ∏è Deleted: {file}")
                    except Exception as delete_error:
                        print(f"‚ö†Ô∏è Warning: Could not delete {file}: {delete_error}")

        # Construct full path to the best file
        best_file_path = os.path.join(local_path, best_file)

        print(f"üöÄ Loading LoRA: {best_file_path}")
        if not hasattr(pipe, "load_lora_weights"):
            raise HTTPException(status_code=400, detail="Pipeline doesn't support LoRA")
        # Load the LoRA weights
        pipe.load_lora_weights(local_path,adapter_name=adapter_name)

        return {
            "message": f"Successfully loaded {best_file} from {lora_path}",
            "loaded_file": best_file,
            "local_path": local_path
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to load LoRA: {str(e)}")

class PromptRequest(BaseModel):
    prompt: str
    height: int = 512
    width: int = 512
    num_inference_steps: int = 20
    guidance_scale: float = 7.5
    control_image: Optional[UploadFile] = File(None)  # For ControlNet input
    lora_scales: Optional[Dict[str, float]] = None

import traceback
import asyncio
from concurrent.futures import ThreadPoolExecutor
from fastapi.responses import FileResponse
from io import BytesIO
from fastapi.responses import StreamingResponse
executor = ThreadPoolExecutor()

def generate_image_sync(prompt, height, width, steps, scale,control_image):
    generator = torch.manual_seed(42)
    if lora_scales:
        base_params["cross_attention_kwargs"] = {"lora_scale": lora_scales}
    return pipe(
        prompt=prompt,
        height=height,
        width=width,
        num_inference_steps=steps,
        guidance_scale=scale,
        generator=generator,
        control_image=control_image
    ).images[0]


from fastapi import HTTPException, UploadFile, File
from typing import Dict, Optional
import asyncio
from io import BytesIO
from PIL import Image

@app.post("/generate-image/")
async def generate(request: PromptRequest):
    print("checkpoint 0")
    try:
        print("checkpoint 1")
        print(f"‚öôÔ∏è Generating with ControlNet: {pipe.controlnet is not None}")
        print(f"LoRA scales: {request.lora_scales}")

        # Process control image if provided
        control_image = None
        if request.control_image:
            if pipe.controlnet is None:
                raise HTTPException(
                    status_code=400,
                    detail="Control image provided but no ControlNet loaded"
                )
            control_image = Image.open(BytesIO(await request.control_image.read()))
        print("checkpoint 2")
        # Run in executor
        image = await asyncio.get_event_loop().run_in_executor(
            executor,
            generate_image_sync,
            request.prompt,
            request.height,
            request.width,
            request.num_inference_steps,
            request.guidance_scale,
            request.control_image,  # Passed to sync function
            request.lora_scales
        )
        print("checkpoint 3")
        # Stream response
        img_bytes = BytesIO()
        image.save(img_bytes, format="PNG")
        img_bytes.seek(0)
        return StreamingResponse(
            img_bytes,
            media_type="image/png",
            headers={"Content-Disposition": 'inline; filename="output.png"'}
        )

    except Exception as e:
        print(f"‚ùå Generation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

import psutil

def get_cpu_ram_stats():
    ram = psutil.virtual_memory()
    cpu = psutil.cpu_percent(interval=1)
    return {
        "cpu_percent": cpu,
        "ram_total": ram.total,
        "ram_used": ram.used,
        "ram_percent": ram.percent
    }

import subprocess
import re

def get_gpu_stats():
    try:
        result = subprocess.check_output(
            ['nvidia-smi', '--query-gpu=memory.total,memory.used,utilization.gpu', '--format=csv,nounits,noheader']
        )
        total, used, utilization = map(int, result.decode().strip().split(', '))
        return {
            "gpu_memory_total": total,
            "gpu_memory_used": used,
            "gpu_utilization": utilization
        }
    except Exception as e:
        return {"error": str(e)}

@app.get("/stats")
def get_stats():
    return {
        "cpu_ram": get_cpu_ram_stats(),
        "gpu": get_gpu_stats()
    }

!pip install pyngrok

import torch
import gc
gc.collect()
torch.cuda.empty_cache()

import uvicorn
from pyngrok import ngrok
import nest_asyncio
!ngrok authtoken 2xNBjsJZTEMSyZIjuNauqUwXAxK_3YEAxPqFauzLBSHA8EJvn
# Allow async in Colab
nest_asyncio.apply()

# Start ngrok tunnel
public_url = ngrok.connect(8000)
print("Public URL:", public_url)

# Run FastAPI server
uvicorn.run(app, host="0.0.0.0", port=8000)
